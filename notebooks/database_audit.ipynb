{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Soft Power Database Audit\n",
    "\n",
    "Comprehensive audit of all database tables and metrics, with focus on:\n",
    "- Documents and their distribution\n",
    "- Event hierarchies (clusters, canonical events, daily mentions, summaries)\n",
    "- Country analysis (influencers and recipients from config.yaml)\n",
    "- Embeddings and vector stores\n",
    "- Date coverage and temporal analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nfrom pathlib import Path\n\n# ============================================================\n# DATABASE CONNECTION CONFIGURATION\n# ============================================================\n# IMPORTANT: Set environment variables BEFORE any other imports\n# to ensure the database module uses the correct host.\n#\n# When running Jupyter OUTSIDE Docker: use 'localhost'\n# When running Jupyter INSIDE Docker: use 'softpower_db'\n# ============================================================\n\n# Set DB_HOST before any imports - this must come first!\nos.environ['DB_HOST'] = 'localhost'  # Change to 'softpower_db' if running inside Docker\nos.environ.pop('DATABASE_URL', None)  # Clear any cached DATABASE_URL\n\n# Add project root to path\nproject_root = Path.cwd().parent\nsys.path.insert(0, str(project_root))\n\nimport pandas as pd\nimport numpy as np\nimport yaml\nfrom datetime import datetime, date\nfrom sqlalchemy import text, func\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Force reload of database module to pick up new environment\nimport importlib\nif 'shared.database.database' in sys.modules:\n    # Clear existing db manager to force reconnection\n    import shared.database.database as db_module\n    db_module._db_manager = None\n    importlib.reload(db_module)\n\n# Now import the database functions\nfrom shared.database.database import get_engine, get_session\n\n# Load config\nwith open(project_root / 'shared/config/config.yaml', 'r') as f:\n    config = yaml.safe_load(f)\n\nINFLUENCERS = config.get('influencers', [])\nRECIPIENTS = config.get('recipients', [])\nCATEGORIES = config.get('categories', [])\n\nprint(\"Configuration Loaded\")\nprint(f\"  Influencing Countries: {INFLUENCERS}\")\nprint(f\"  Recipient Countries: {len(RECIPIENTS)} countries\")\nprint(f\"  Categories: {CATEGORIES}\")\nprint(f\"  DB Host: {os.environ.get('DB_HOST', 'NOT SET')}\")\n\n# Test connection\ntry:\n    from shared.database.database import health_check\n    if health_check():\n        print(\"  Database Connection: OK\")\n    else:\n        print(\"  Database Connection: FAILED\")\nexcept Exception as e:\n    print(f\"  Database Connection: ERROR - {e}\")"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to run queries\n",
    "def run_query(query, params=None):\n",
    "    \"\"\"Run a SQL query and return results as DataFrame.\"\"\"\n",
    "    engine = get_engine()\n",
    "    with engine.connect() as conn:\n",
    "        result = pd.read_sql(text(query), conn, params=params)\n",
    "    return result\n",
    "\n",
    "def run_scalar(query, params=None):\n",
    "    \"\"\"Run a SQL query and return scalar result.\"\"\"\n",
    "    engine = get_engine()\n",
    "    with engine.connect() as conn:\n",
    "        result = conn.execute(text(query), params or {}).scalar()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Table Overview\n",
    "\n",
    "Summary of all tables and their row counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:shared.database.database:Database connection attempt 1 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "ERROR:shared.database.database:Database connection attempt 2 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "ERROR:shared.database.database:Database connection attempt 3 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to database after 3 attempts: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3301\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3281\u001b[0m \n\u001b[1;32m   3282\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3299\u001b[0m \n\u001b[1;32m   3300\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1266\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    713\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    178\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:899\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    900\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    896\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:629\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:136\u001b[0m, in \u001b[0;36mDatabaseManager._setup_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Test the connection - use text() for raw SQL\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mconnect() \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m    137\u001b[0m     conn\u001b[39m.\u001b[39mexecute(text(\u001b[39m\"\u001b[39m\u001b[39mSELECT 1\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3277\u001b[0m, in \u001b[0;36mEngine.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \n\u001b[1;32m   3257\u001b[0m \u001b[39mThe :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3274\u001b[0m \n\u001b[1;32m   3275\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3277\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection_cls(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:145\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 145\u001b[0m     Connection\u001b[39m.\u001b[39;49m_handle_dbapi_exception_noconnection(\n\u001b[1;32m    146\u001b[0m         err, dialect, engine\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2440\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[39massert\u001b[39;00m sqlalchemy_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2440\u001b[0m     \u001b[39mraise\u001b[39;00m sqlalchemy_exception\u001b[39m.\u001b[39mwith_traceback(exc_info[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3301\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3281\u001b[0m \n\u001b[1;32m   3282\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3299\u001b[0m \n\u001b[1;32m   3300\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1266\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    713\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    178\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:899\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    900\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    896\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:629\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Get all table counts\u001b[39;00m\n\u001b[1;32m      2\u001b[0m tables_query \u001b[39m=\u001b[39m \u001b[39m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mSELECT \u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39m    schemaname,\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mORDER BY n_live_tup DESC\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\"\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m tables_df \u001b[39m=\u001b[39m run_query(tables_query)\n\u001b[1;32m     12\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m60\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTABLE OVERVIEW\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m, in \u001b[0;36mrun_query\u001b[0;34m(query, params)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mrun_query\u001b[39m(query, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      3\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a SQL query and return results as DataFrame.\"\"\"\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m     engine \u001b[39m=\u001b[39m get_engine()\n\u001b[1;32m      5\u001b[0m     \u001b[39mwith\u001b[39;00m engine\u001b[39m.\u001b[39mconnect() \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m      6\u001b[0m         result \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_sql(text(query), conn, params\u001b[39m=\u001b[39mparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:295\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_engine\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Engine:\n\u001b[1;32m    294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the SQLAlchemy engine instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m get_db_manager()\u001b[39m.\u001b[39mengine\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:268\u001b[0m, in \u001b[0;36mget_db_manager\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mglobal\u001b[39;00m _db_manager\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m _db_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     _db_manager \u001b[39m=\u001b[39m DatabaseManager()\n\u001b[1;32m    269\u001b[0m \u001b[39mreturn\u001b[39;00m _db_manager\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:47\u001b[0m, in \u001b[0;36mDatabaseManager.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_retries \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_delay \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_connection()\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_event_listeners()\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:154\u001b[0m, in \u001b[0;36mDatabaseManager._setup_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_delay \u001b[39m*\u001b[39m (attempt \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))  \u001b[39m# Exponential backoff\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to connect to database after \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_retries\u001b[39m}\u001b[39;00m\u001b[39m attempts: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Failed to connect to database after 3 attempts: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Get all table counts\n",
    "tables_query = \"\"\"\n",
    "SELECT \n",
    "    schemaname,\n",
    "    relname as table_name,\n",
    "    n_live_tup as row_count\n",
    "FROM pg_stat_user_tables\n",
    "ORDER BY n_live_tup DESC\n",
    "\"\"\"\n",
    "\n",
    "tables_df = run_query(tables_query)\n",
    "print(\"=\" * 60)\n",
    "print(\"TABLE OVERVIEW\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nTotal tables: {len(tables_df)}\")\n",
    "print(f\"Total rows across all tables: {tables_df['row_count'].sum():,}\")\n",
    "print(\"\\n\")\n",
    "tables_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Documents Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:shared.database.database:Database connection attempt 1 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "DOCUMENTS TABLE ANALYSIS\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:shared.database.database:Database connection attempt 2 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n",
      "ERROR:shared.database.database:Database connection attempt 3 failed: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\n",
      "(Background on this error at: https://sqlalche.me/e/20/e3q8)\n"
     ]
    },
    {
     "ename": "ConnectionError",
     "evalue": "Failed to connect to database after 3 attempts: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3301\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3281\u001b[0m \n\u001b[1;32m   3282\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3299\u001b[0m \n\u001b[1;32m   3300\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1266\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    713\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    178\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:899\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    900\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    896\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:629\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: could not translate host name \"softpower_db\" to address: Name or service not known\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:136\u001b[0m, in \u001b[0;36mDatabaseManager._setup_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[39m# Test the connection - use text() for raw SQL\u001b[39;00m\n\u001b[0;32m--> 136\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine\u001b[39m.\u001b[39;49mconnect() \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m    137\u001b[0m     conn\u001b[39m.\u001b[39mexecute(text(\u001b[39m\"\u001b[39m\u001b[39mSELECT 1\u001b[39m\u001b[39m\"\u001b[39m))\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3277\u001b[0m, in \u001b[0;36mEngine.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3255\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a new :class:`_engine.Connection` object.\u001b[39;00m\n\u001b[1;32m   3256\u001b[0m \n\u001b[1;32m   3257\u001b[0m \u001b[39mThe :class:`_engine.Connection` acts as a Python context manager, so\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3274\u001b[0m \n\u001b[1;32m   3275\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3277\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_connection_cls(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:145\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m--> 145\u001b[0m     Connection\u001b[39m.\u001b[39;49m_handle_dbapi_exception_noconnection(\n\u001b[1;32m    146\u001b[0m         err, dialect, engine\n\u001b[1;32m    147\u001b[0m     )\n\u001b[1;32m    148\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:2440\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception_noconnection\u001b[0;34m(cls, e, dialect, engine, is_disconnect, invalidate_pool_on_disconnect, is_pre_ping)\u001b[0m\n\u001b[1;32m   2439\u001b[0m     \u001b[39massert\u001b[39;00m sqlalchemy_exception \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m-> 2440\u001b[0m     \u001b[39mraise\u001b[39;00m sqlalchemy_exception\u001b[39m.\u001b[39mwith_traceback(exc_info[\u001b[39m2\u001b[39m]) \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39me\u001b[39;00m\n\u001b[1;32m   2441\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:143\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[0;34m(self, engine, connection, _has_events, _allow_revalidate, _allow_autobegin)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dbapi_connection \u001b[39m=\u001b[39m engine\u001b[39m.\u001b[39;49mraw_connection()\n\u001b[1;32m    144\u001b[0m \u001b[39mexcept\u001b[39;00m dialect\u001b[39m.\u001b[39mloaded_dbapi\u001b[39m.\u001b[39mError \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/base.py:3301\u001b[0m, in \u001b[0;36mEngine.raw_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3280\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a \"raw\" DBAPI connection from the connection pool.\u001b[39;00m\n\u001b[1;32m   3281\u001b[0m \n\u001b[1;32m   3282\u001b[0m \u001b[39mThe returned object is a proxied version of the DBAPI\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3299\u001b[0m \n\u001b[1;32m   3300\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 3301\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpool\u001b[39m.\u001b[39;49mconnect()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:447\u001b[0m, in \u001b[0;36mPool.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    440\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Return a DBAPI connection from the pool.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m \n\u001b[1;32m    442\u001b[0m \u001b[39mThe connection is instrumented such that when its\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    445\u001b[0m \n\u001b[1;32m    446\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 447\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionFairy\u001b[39m.\u001b[39;49m_checkout(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:1264\u001b[0m, in \u001b[0;36m_ConnectionFairy._checkout\u001b[0;34m(cls, pool, threadconns, fairy)\u001b[0m\n\u001b[1;32m   1263\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m fairy:\n\u001b[0;32m-> 1264\u001b[0m     fairy \u001b[39m=\u001b[39m _ConnectionRecord\u001b[39m.\u001b[39;49mcheckout(pool)\n\u001b[1;32m   1266\u001b[0m     \u001b[39mif\u001b[39;00m threadconns \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:711\u001b[0m, in \u001b[0;36m_ConnectionRecord.checkout\u001b[0;34m(cls, pool)\u001b[0m\n\u001b[1;32m    710\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 711\u001b[0m     rec \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_do_get()\n\u001b[1;32m    713\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:177\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m--> 177\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    178\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dec_overflow()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/impl.py:175\u001b[0m, in \u001b[0;36mQueuePool._do_get\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 175\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_create_connection()\n\u001b[1;32m    176\u001b[0m \u001b[39mexcept\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:388\u001b[0m, in \u001b[0;36mPool._create_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Called by subclasses to create a new ConnectionRecord.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 388\u001b[0m \u001b[39mreturn\u001b[39;00m _ConnectionRecord(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:673\u001b[0m, in \u001b[0;36m_ConnectionRecord.__init__\u001b[0;34m(self, pool, connect)\u001b[0m\n\u001b[1;32m    672\u001b[0m \u001b[39mif\u001b[39;00m connect:\n\u001b[0;32m--> 673\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__connect()\n\u001b[1;32m    674\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalize_callback \u001b[39m=\u001b[39m deque()\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:899\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 899\u001b[0m     \u001b[39mwith\u001b[39;00m util\u001b[39m.\u001b[39msafe_reraise():\n\u001b[1;32m    900\u001b[0m         pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mError on connect(): \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, e)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/util/langhelpers.py:224\u001b[0m, in \u001b[0;36msafe_reraise.__exit__\u001b[0;34m(self, type_, value, traceback)\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exc_info \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m  \u001b[39m# remove potential circular references\u001b[39;00m\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mraise\u001b[39;00m exc_value\u001b[39m.\u001b[39mwith_traceback(exc_tb)\n\u001b[1;32m    225\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/pool/base.py:895\u001b[0m, in \u001b[0;36m_ConnectionRecord.__connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstarttime \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[0;32m--> 895\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdbapi_connection \u001b[39m=\u001b[39m connection \u001b[39m=\u001b[39m pool\u001b[39m.\u001b[39;49m_invoke_creator(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    896\u001b[0m pool\u001b[39m.\u001b[39mlogger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mCreated new connection \u001b[39m\u001b[39m%r\u001b[39;00m\u001b[39m\"\u001b[39m, connection)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/create.py:661\u001b[0m, in \u001b[0;36mcreate_engine.<locals>.connect\u001b[0;34m(connection_record)\u001b[0m\n\u001b[1;32m    659\u001b[0m             \u001b[39mreturn\u001b[39;00m connection\n\u001b[0;32m--> 661\u001b[0m \u001b[39mreturn\u001b[39;00m dialect\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/sqlalchemy/engine/default.py:629\u001b[0m, in \u001b[0;36mDefaultDialect.connect\u001b[0;34m(self, *cargs, **cparams)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mconnect\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mcargs: Any, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcparams: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DBAPIConnection:\n\u001b[1;32m    628\u001b[0m     \u001b[39m# inherits the docstring from interfaces.Dialect.connect\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mloaded_dbapi\u001b[39m.\u001b[39;49mconnect(\u001b[39m*\u001b[39;49mcargs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mcparams)\n",
      "File \u001b[0;32m~/SP_Streamlit/venv/lib/python3.10/site-packages/psycopg2/__init__.py:122\u001b[0m, in \u001b[0;36mconnect\u001b[0;34m(dsn, connection_factory, cursor_factory, **kwargs)\u001b[0m\n\u001b[1;32m    121\u001b[0m dsn \u001b[39m=\u001b[39m _ext\u001b[39m.\u001b[39mmake_dsn(dsn, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m--> 122\u001b[0m conn \u001b[39m=\u001b[39m _connect(dsn, connection_factory\u001b[39m=\u001b[39;49mconnection_factory, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwasync)\n\u001b[1;32m    123\u001b[0m \u001b[39mif\u001b[39;00m cursor_factory \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mOperationalError\u001b[0m: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m60\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[39m# Basic counts\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m doc_count \u001b[39m=\u001b[39m run_scalar(\u001b[39m\"\u001b[39;49m\u001b[39mSELECT COUNT(*) FROM documents\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m      7\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mTotal Documents: \u001b[39m\u001b[39m{\u001b[39;00mdoc_count\u001b[39m:\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      9\u001b[0m \u001b[39m# Date range\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mrun_scalar\u001b[0;34m(query, params)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mrun_scalar\u001b[39m(query, params\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     10\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Run a SQL query and return scalar result.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     engine \u001b[39m=\u001b[39m get_engine()\n\u001b[1;32m     12\u001b[0m     \u001b[39mwith\u001b[39;00m engine\u001b[39m.\u001b[39mconnect() \u001b[39mas\u001b[39;00m conn:\n\u001b[1;32m     13\u001b[0m         result \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39mexecute(text(query), params \u001b[39mor\u001b[39;00m {})\u001b[39m.\u001b[39mscalar()\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:295\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m()\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mget_engine\u001b[39m() \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Engine:\n\u001b[1;32m    294\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Get the SQLAlchemy engine instance.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 295\u001b[0m     \u001b[39mreturn\u001b[39;00m get_db_manager()\u001b[39m.\u001b[39mengine\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:268\u001b[0m, in \u001b[0;36mget_db_manager\u001b[0;34m()\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[39mglobal\u001b[39;00m _db_manager\n\u001b[1;32m    267\u001b[0m \u001b[39mif\u001b[39;00m _db_manager \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 268\u001b[0m     _db_manager \u001b[39m=\u001b[39m DatabaseManager()\n\u001b[1;32m    269\u001b[0m \u001b[39mreturn\u001b[39;00m _db_manager\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:47\u001b[0m, in \u001b[0;36mDatabaseManager.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_retries \u001b[39m=\u001b[39m \u001b[39m3\u001b[39m\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_delay \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m  \u001b[39m# seconds\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setup_connection()\n\u001b[1;32m     48\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setup_event_listeners()\n",
      "File \u001b[0;32m~/SP_Streamlit/shared/database/database.py:154\u001b[0m, in \u001b[0;36mDatabaseManager._setup_connection\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retry_delay \u001b[39m*\u001b[39m (attempt \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))  \u001b[39m# Exponential backoff\u001b[39;00m\n\u001b[1;32m    153\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mFailed to connect to database after \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_connection_retries\u001b[39m}\u001b[39;00m\u001b[39m attempts: \u001b[39m\u001b[39m{\u001b[39;00me\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mConnectionError\u001b[0m: Failed to connect to database after 3 attempts: (psycopg2.OperationalError) could not translate host name \"softpower_db\" to address: Name or service not known\n\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DOCUMENTS TABLE ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic counts\n",
    "doc_count = run_scalar(\"SELECT COUNT(*) FROM documents\")\n",
    "print(f\"\\nTotal Documents: {doc_count:,}\")\n",
    "\n",
    "# Date range\n",
    "date_range = run_query(\"\"\"\n",
    "    SELECT \n",
    "        MIN(date) as earliest_date,\n",
    "        MAX(date) as latest_date,\n",
    "        COUNT(DISTINCT date) as unique_dates\n",
    "    FROM documents\n",
    "    WHERE date IS NOT NULL\n",
    "\"\"\")\n",
    "print(f\"\\nDate Range:\")\n",
    "print(f\"  Earliest: {date_range['earliest_date'].iloc[0]}\")\n",
    "print(f\"  Latest: {date_range['latest_date'].iloc[0]}\")\n",
    "print(f\"  Unique Dates: {date_range['unique_dates'].iloc[0]:,}\")\n",
    "\n",
    "# Documents by month\n",
    "docs_by_month = run_query(\"\"\"\n",
    "    SELECT \n",
    "        DATE_TRUNC('month', date)::date as month,\n",
    "        COUNT(*) as doc_count\n",
    "    FROM documents\n",
    "    WHERE date IS NOT NULL\n",
    "    GROUP BY DATE_TRUNC('month', date)\n",
    "    ORDER BY month\n",
    "\"\"\")\n",
    "print(f\"\\nDocuments by Month:\")\n",
    "docs_by_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents by initiating country (from config influencers)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DOCUMENTS BY INITIATING COUNTRY (Influencers)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "init_country_query = \"\"\"\n",
    "    SELECT \n",
    "        initiating_country,\n",
    "        COUNT(*) as doc_count,\n",
    "        MIN(date) as earliest_date,\n",
    "        MAX(date) as latest_date\n",
    "    FROM documents\n",
    "    WHERE initiating_country IS NOT NULL\n",
    "    GROUP BY initiating_country\n",
    "    ORDER BY doc_count DESC\n",
    "\"\"\"\n",
    "init_countries = run_query(init_country_query)\n",
    "\n",
    "# Highlight config influencers\n",
    "init_countries['in_config'] = init_countries['initiating_country'].isin(INFLUENCERS)\n",
    "print(\"\\nAll Initiating Countries:\")\n",
    "display(init_countries)\n",
    "\n",
    "# Summary for config influencers\n",
    "config_influencers = init_countries[init_countries['in_config']]\n",
    "print(f\"\\nConfig Influencers Coverage:\")\n",
    "print(f\"  Total docs from config influencers: {config_influencers['doc_count'].sum():,}\")\n",
    "print(f\"  Percentage of total: {config_influencers['doc_count'].sum() / doc_count * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents by recipient country (from config recipients)\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DOCUMENTS BY RECIPIENT COUNTRY (Recipients)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "rec_country_query = \"\"\"\n",
    "    SELECT \n",
    "        recipient_country,\n",
    "        COUNT(*) as doc_count,\n",
    "        MIN(date) as earliest_date,\n",
    "        MAX(date) as latest_date\n",
    "    FROM documents\n",
    "    WHERE recipient_country IS NOT NULL\n",
    "    GROUP BY recipient_country\n",
    "    ORDER BY doc_count DESC\n",
    "\"\"\"\n",
    "rec_countries = run_query(rec_country_query)\n",
    "\n",
    "# Highlight config recipients\n",
    "rec_countries['in_config'] = rec_countries['recipient_country'].isin(RECIPIENTS)\n",
    "print(\"\\nTop 30 Recipient Countries:\")\n",
    "display(rec_countries.head(30))\n",
    "\n",
    "# Summary for config recipients\n",
    "config_recipients = rec_countries[rec_countries['in_config']]\n",
    "print(f\"\\nConfig Recipients Coverage:\")\n",
    "print(f\"  Total docs to config recipients: {config_recipients['doc_count'].sum():,}\")\n",
    "print(f\"  Percentage of total: {config_recipients['doc_count'].sum() / doc_count * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Documents by category\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DOCUMENTS BY CATEGORY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "category_query = \"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as doc_count\n",
    "    FROM documents\n",
    "    WHERE category IS NOT NULL\n",
    "    GROUP BY category\n",
    "    ORDER BY doc_count DESC\n",
    "\"\"\"\n",
    "categories = run_query(category_query)\n",
    "categories['in_config'] = categories['category'].isin(CATEGORIES)\n",
    "print(\"\\nCategories:\")\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salience analysis\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"SALIENCE DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "salience_query = \"\"\"\n",
    "    SELECT \n",
    "        salience_bool,\n",
    "        COUNT(*) as doc_count\n",
    "    FROM documents\n",
    "    GROUP BY salience_bool\n",
    "    ORDER BY doc_count DESC\n",
    "\"\"\"\n",
    "salience_dist = run_query(salience_query)\n",
    "print(\"\\nSalience Distribution:\")\n",
    "salience_dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Normalized Relationship Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"NORMALIZED RELATIONSHIP TABLES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Categories table\n",
    "cat_count = run_scalar(\"SELECT COUNT(*) FROM categories\")\n",
    "cat_unique = run_scalar(\"SELECT COUNT(DISTINCT category) FROM categories\")\n",
    "print(f\"\\nCategories Table:\")\n",
    "print(f\"  Total rows: {cat_count:,}\")\n",
    "print(f\"  Unique categories: {cat_unique}\")\n",
    "\n",
    "# Subcategories table\n",
    "subcat_count = run_scalar(\"SELECT COUNT(*) FROM subcategories\")\n",
    "subcat_unique = run_scalar(\"SELECT COUNT(DISTINCT subcategory) FROM subcategories\")\n",
    "print(f\"\\nSubcategories Table:\")\n",
    "print(f\"  Total rows: {subcat_count:,}\")\n",
    "print(f\"  Unique subcategories: {subcat_unique}\")\n",
    "\n",
    "# Initiating countries table\n",
    "init_count = run_scalar(\"SELECT COUNT(*) FROM initiating_countries\")\n",
    "init_unique = run_scalar(\"SELECT COUNT(DISTINCT initiating_country) FROM initiating_countries\")\n",
    "print(f\"\\nInitiating Countries Table:\")\n",
    "print(f\"  Total rows: {init_count:,}\")\n",
    "print(f\"  Unique countries: {init_unique}\")\n",
    "\n",
    "# Recipient countries table\n",
    "rec_count = run_scalar(\"SELECT COUNT(*) FROM recipient_countries\")\n",
    "rec_unique = run_scalar(\"SELECT COUNT(DISTINCT recipient_country) FROM recipient_countries\")\n",
    "print(f\"\\nRecipient Countries Table:\")\n",
    "print(f\"  Total rows: {rec_count:,}\")\n",
    "print(f\"  Unique countries: {rec_unique}\")\n",
    "\n",
    "# Raw events table\n",
    "raw_events_count = run_scalar(\"SELECT COUNT(*) FROM raw_events\")\n",
    "raw_events_unique = run_scalar(\"SELECT COUNT(DISTINCT event_name) FROM raw_events\")\n",
    "print(f\"\\nRaw Events Table:\")\n",
    "print(f\"  Total rows: {raw_events_count:,}\")\n",
    "print(f\"  Unique event names: {raw_events_unique:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Event Hierarchy Analysis\n",
    "\n",
    "The event processing pipeline creates multiple levels of event aggregation:\n",
    "1. **Event Clusters** - Initial DBSCAN clustering by country/date\n",
    "2. **Canonical Events** - LLM-validated unique events\n",
    "3. **Daily Event Mentions** - Daily mentions of canonical events\n",
    "4. **Event Summaries** - Period-based summaries (daily/weekly/monthly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EVENT CLUSTERS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Check if table exists and has data\n",
    "try:\n",
    "    cluster_count = run_scalar(\"SELECT COUNT(*) FROM event_clusters\")\n",
    "    print(f\"\\nTotal Event Clusters: {cluster_count:,}\")\n",
    "    \n",
    "    if cluster_count > 0:\n",
    "        # Clusters by country\n",
    "        clusters_by_country = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                COUNT(*) as cluster_count,\n",
    "                SUM(cluster_size) as total_events,\n",
    "                MIN(cluster_date) as earliest_date,\n",
    "                MAX(cluster_date) as latest_date,\n",
    "                SUM(CASE WHEN processed THEN 1 ELSE 0 END) as processed_count,\n",
    "                SUM(CASE WHEN llm_deconflicted THEN 1 ELSE 0 END) as llm_validated_count\n",
    "            FROM event_clusters\n",
    "            GROUP BY initiating_country\n",
    "            ORDER BY cluster_count DESC\n",
    "        \"\"\")\n",
    "        clusters_by_country['in_config'] = clusters_by_country['initiating_country'].isin(INFLUENCERS)\n",
    "        print(\"\\nClusters by Country:\")\n",
    "        display(clusters_by_country)\n",
    "        \n",
    "        # Clusters by month\n",
    "        clusters_by_month = run_query(\"\"\"\n",
    "            SELECT \n",
    "                DATE_TRUNC('month', cluster_date)::date as month,\n",
    "                COUNT(*) as cluster_count,\n",
    "                SUM(cluster_size) as total_events\n",
    "            FROM event_clusters\n",
    "            GROUP BY DATE_TRUNC('month', cluster_date)\n",
    "            ORDER BY month\n",
    "        \"\"\")\n",
    "        print(\"\\nClusters by Month:\")\n",
    "        display(clusters_by_month)\n",
    "except Exception as e:\n",
    "    print(f\"Event clusters table not available or empty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"CANONICAL EVENTS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    canonical_count = run_scalar(\"SELECT COUNT(*) FROM canonical_events\")\n",
    "    print(f\"\\nTotal Canonical Events: {canonical_count:,}\")\n",
    "    \n",
    "    if canonical_count > 0:\n",
    "        # Master vs child events\n",
    "        master_count = run_scalar(\"SELECT COUNT(*) FROM canonical_events WHERE master_event_id IS NULL\")\n",
    "        child_count = run_scalar(\"SELECT COUNT(*) FROM canonical_events WHERE master_event_id IS NOT NULL\")\n",
    "        print(f\"  Master events: {master_count:,}\")\n",
    "        print(f\"  Child events: {child_count:,}\")\n",
    "        \n",
    "        # By country\n",
    "        canonical_by_country = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                COUNT(*) as event_count,\n",
    "                SUM(total_articles) as total_articles,\n",
    "                AVG(total_mention_days) as avg_mention_days,\n",
    "                MIN(first_mention_date) as earliest_mention,\n",
    "                MAX(last_mention_date) as latest_mention\n",
    "            FROM canonical_events\n",
    "            GROUP BY initiating_country\n",
    "            ORDER BY event_count DESC\n",
    "        \"\"\")\n",
    "        canonical_by_country['in_config'] = canonical_by_country['initiating_country'].isin(INFLUENCERS)\n",
    "        print(\"\\nCanonical Events by Country:\")\n",
    "        display(canonical_by_country)\n",
    "        \n",
    "        # Story phase distribution\n",
    "        story_phase = run_query(\"\"\"\n",
    "            SELECT \n",
    "                story_phase,\n",
    "                COUNT(*) as event_count\n",
    "            FROM canonical_events\n",
    "            GROUP BY story_phase\n",
    "            ORDER BY event_count DESC\n",
    "        \"\"\")\n",
    "        print(\"\\nStory Phase Distribution:\")\n",
    "        display(story_phase)\n",
    "        \n",
    "        # Materiality scores\n",
    "        materiality = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                COUNT(*) as events_with_score,\n",
    "                AVG(material_score) as avg_score,\n",
    "                MIN(material_score) as min_score,\n",
    "                MAX(material_score) as max_score\n",
    "            FROM canonical_events\n",
    "            WHERE material_score IS NOT NULL\n",
    "            GROUP BY initiating_country\n",
    "            ORDER BY avg_score DESC\n",
    "        \"\"\")\n",
    "        print(\"\\nMateriality Scores by Country:\")\n",
    "        display(materiality)\n",
    "except Exception as e:\n",
    "    print(f\"Canonical events table not available or empty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DAILY EVENT MENTIONS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    mention_count = run_scalar(\"SELECT COUNT(*) FROM daily_event_mentions\")\n",
    "    print(f\"\\nTotal Daily Mentions: {mention_count:,}\")\n",
    "    \n",
    "    if mention_count > 0:\n",
    "        # By country\n",
    "        mentions_by_country = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                COUNT(*) as mention_count,\n",
    "                SUM(article_count) as total_articles,\n",
    "                COUNT(DISTINCT mention_date) as unique_dates,\n",
    "                MIN(mention_date) as earliest_date,\n",
    "                MAX(mention_date) as latest_date\n",
    "            FROM daily_event_mentions\n",
    "            GROUP BY initiating_country\n",
    "            ORDER BY mention_count DESC\n",
    "        \"\"\")\n",
    "        mentions_by_country['in_config'] = mentions_by_country['initiating_country'].isin(INFLUENCERS)\n",
    "        print(\"\\nDaily Mentions by Country:\")\n",
    "        display(mentions_by_country)\n",
    "        \n",
    "        # By month\n",
    "        mentions_by_month = run_query(\"\"\"\n",
    "            SELECT \n",
    "                DATE_TRUNC('month', mention_date)::date as month,\n",
    "                COUNT(*) as mention_count,\n",
    "                SUM(article_count) as total_articles\n",
    "            FROM daily_event_mentions\n",
    "            GROUP BY DATE_TRUNC('month', mention_date)\n",
    "            ORDER BY month\n",
    "        \"\"\")\n",
    "        print(\"\\nDaily Mentions by Month:\")\n",
    "        display(mentions_by_month)\n",
    "        \n",
    "        # News intensity distribution\n",
    "        intensity = run_query(\"\"\"\n",
    "            SELECT \n",
    "                news_intensity,\n",
    "                COUNT(*) as mention_count\n",
    "            FROM daily_event_mentions\n",
    "            GROUP BY news_intensity\n",
    "            ORDER BY mention_count DESC\n",
    "        \"\"\")\n",
    "        print(\"\\nNews Intensity Distribution:\")\n",
    "        display(intensity)\n",
    "except Exception as e:\n",
    "    print(f\"Daily event mentions table not available or empty: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"EVENT SUMMARIES ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    summary_count = run_scalar(\"SELECT COUNT(*) FROM event_summaries\")\n",
    "    print(f\"\\nTotal Event Summaries: {summary_count:,}\")\n",
    "    \n",
    "    if summary_count > 0:\n",
    "        # By period type\n",
    "        summaries_by_type = run_query(\"\"\"\n",
    "            SELECT \n",
    "                period_type,\n",
    "                COUNT(*) as summary_count,\n",
    "                MIN(period_start) as earliest_period,\n",
    "                MAX(period_end) as latest_period\n",
    "            FROM event_summaries\n",
    "            GROUP BY period_type\n",
    "            ORDER BY summary_count DESC\n",
    "        \"\"\")\n",
    "        print(\"\\nSummaries by Period Type:\")\n",
    "        display(summaries_by_type)\n",
    "        \n",
    "        # By country\n",
    "        summaries_by_country = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                COUNT(*) as summary_count,\n",
    "                SUM(total_documents_across_sources) as total_docs,\n",
    "                MIN(period_start) as earliest_period,\n",
    "                MAX(period_end) as latest_period\n",
    "            FROM event_summaries\n",
    "            GROUP BY initiating_country\n",
    "            ORDER BY summary_count DESC\n",
    "        \"\"\")\n",
    "        summaries_by_country['in_config'] = summaries_by_country['initiating_country'].isin(INFLUENCERS)\n",
    "        print(\"\\nSummaries by Country:\")\n",
    "        display(summaries_by_country)\n",
    "        \n",
    "        # Status distribution\n",
    "        status_dist = run_query(\"\"\"\n",
    "            SELECT \n",
    "                status,\n",
    "                COUNT(*) as summary_count\n",
    "            FROM event_summaries\n",
    "            GROUP BY status\n",
    "        \"\"\")\n",
    "        print(\"\\nStatus Distribution:\")\n",
    "        display(status_dist)\n",
    "except Exception as e:\n",
    "    print(f\"Event summaries table not available or empty: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Embeddings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EMBEDDINGS ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    # Collections\n",
    "    collections = run_query(\"\"\"\n",
    "        SELECT \n",
    "            c.name as collection_name,\n",
    "            c.uuid,\n",
    "            COUNT(e.uuid) as embedding_count\n",
    "        FROM langchain_pg_collection c\n",
    "        LEFT JOIN langchain_pg_embedding e ON c.uuid = e.collection_id\n",
    "        GROUP BY c.name, c.uuid\n",
    "        ORDER BY embedding_count DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    total_embeddings = collections['embedding_count'].sum()\n",
    "    print(f\"\\nTotal Embedding Collections: {len(collections)}\")\n",
    "    print(f\"Total Embeddings: {total_embeddings:,}\")\n",
    "    print(\"\\nCollections:\")\n",
    "    display(collections)\n",
    "except Exception as e:\n",
    "    print(f\"Embeddings tables not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Country-Focused Analysis (Influencers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"INFLUENCER COUNTRY DEEP DIVE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for country in INFLUENCERS:\n",
    "    print(f\"\\n{'=' * 40}\")\n",
    "    print(f\"  {country.upper()}\")\n",
    "    print(f\"{'=' * 40}\")\n",
    "    \n",
    "    # Documents\n",
    "    doc_stats = run_query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_docs,\n",
    "            MIN(date) as earliest_date,\n",
    "            MAX(date) as latest_date,\n",
    "            COUNT(DISTINCT date) as unique_dates\n",
    "        FROM documents\n",
    "        WHERE initiating_country = :country\n",
    "    \"\"\", {'country': country})\n",
    "    print(f\"\\n  Documents: {doc_stats['total_docs'].iloc[0]:,}\")\n",
    "    print(f\"  Date Range: {doc_stats['earliest_date'].iloc[0]} to {doc_stats['latest_date'].iloc[0]}\")\n",
    "    \n",
    "    # Top recipients\n",
    "    top_recipients = run_query(\"\"\"\n",
    "        SELECT \n",
    "            recipient_country,\n",
    "            COUNT(*) as doc_count\n",
    "        FROM documents\n",
    "        WHERE initiating_country = :country\n",
    "        AND recipient_country IS NOT NULL\n",
    "        GROUP BY recipient_country\n",
    "        ORDER BY doc_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\", {'country': country})\n",
    "    print(f\"\\n  Top 10 Recipient Countries:\")\n",
    "    for _, row in top_recipients.iterrows():\n",
    "        in_config = '(*)' if row['recipient_country'] in RECIPIENTS else ''\n",
    "        print(f\"    {row['recipient_country']}: {row['doc_count']:,} {in_config}\")\n",
    "    \n",
    "    # Categories\n",
    "    categories = run_query(\"\"\"\n",
    "        SELECT \n",
    "            category,\n",
    "            COUNT(*) as doc_count\n",
    "        FROM documents\n",
    "        WHERE initiating_country = :country\n",
    "        AND category IS NOT NULL\n",
    "        GROUP BY category\n",
    "        ORDER BY doc_count DESC\n",
    "    \"\"\", {'country': country})\n",
    "    print(f\"\\n  Categories:\")\n",
    "    for _, row in categories.iterrows():\n",
    "        print(f\"    {row['category']}: {row['doc_count']:,}\")\n",
    "    \n",
    "    # Canonical events (if available)\n",
    "    try:\n",
    "        event_stats = run_query(\"\"\"\n",
    "            SELECT \n",
    "                COUNT(*) as total_events,\n",
    "                SUM(total_articles) as total_articles,\n",
    "                AVG(material_score) as avg_materiality\n",
    "            FROM canonical_events\n",
    "            WHERE initiating_country = :country\n",
    "        \"\"\", {'country': country})\n",
    "        print(f\"\\n  Canonical Events: {event_stats['total_events'].iloc[0]:,}\")\n",
    "        print(f\"  Total Articles: {event_stats['total_articles'].iloc[0]:,}\")\n",
    "        if event_stats['avg_materiality'].iloc[0]:\n",
    "            print(f\"  Avg Materiality Score: {event_stats['avg_materiality'].iloc[0]:.2f}\")\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Country-Focused Analysis (Recipients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"RECIPIENT COUNTRY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Summary for all config recipients\n",
    "recipient_summary = []\n",
    "\n",
    "for country in RECIPIENTS:\n",
    "    stats = run_query(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total_docs,\n",
    "            COUNT(DISTINCT initiating_country) as unique_influencers,\n",
    "            MIN(date) as earliest_date,\n",
    "            MAX(date) as latest_date\n",
    "        FROM documents\n",
    "        WHERE recipient_country = :country\n",
    "    \"\"\", {'country': country})\n",
    "    \n",
    "    recipient_summary.append({\n",
    "        'recipient_country': country,\n",
    "        'total_docs': stats['total_docs'].iloc[0],\n",
    "        'unique_influencers': stats['unique_influencers'].iloc[0],\n",
    "        'earliest_date': stats['earliest_date'].iloc[0],\n",
    "        'latest_date': stats['latest_date'].iloc[0]\n",
    "    })\n",
    "\n",
    "recipient_df = pd.DataFrame(recipient_summary)\n",
    "recipient_df = recipient_df.sort_values('total_docs', ascending=False)\n",
    "print(\"\\nRecipient Countries Summary:\")\n",
    "recipient_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Influencer activity in top recipient countries\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"INFLUENCER ACTIVITY BY RECIPIENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "top_recipients = recipient_df.head(10)['recipient_country'].tolist()\n",
    "\n",
    "for recipient in top_recipients:\n",
    "    print(f\"\\n  {recipient}:\")\n",
    "    influencer_activity = run_query(\"\"\"\n",
    "        SELECT \n",
    "            initiating_country,\n",
    "            COUNT(*) as doc_count\n",
    "        FROM documents\n",
    "        WHERE recipient_country = :recipient\n",
    "        AND initiating_country IN :influencers\n",
    "        GROUP BY initiating_country\n",
    "        ORDER BY doc_count DESC\n",
    "    \"\"\", {'recipient': recipient, 'influencers': tuple(INFLUENCERS)})\n",
    "    \n",
    "    for _, row in influencer_activity.iterrows():\n",
    "        print(f\"    {row['initiating_country']}: {row['doc_count']:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Bilateral Relationship Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"BILATERAL RELATIONSHIP SUMMARIES\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "try:\n",
    "    bilateral_count = run_scalar(\"SELECT COUNT(*) FROM bilateral_relationship_summaries\")\n",
    "    print(f\"\\nTotal Bilateral Summaries: {bilateral_count:,}\")\n",
    "    \n",
    "    if bilateral_count > 0:\n",
    "        bilateral_summaries = run_query(\"\"\"\n",
    "            SELECT \n",
    "                initiating_country,\n",
    "                recipient_country,\n",
    "                total_documents,\n",
    "                total_daily_events,\n",
    "                material_score_avg,\n",
    "                first_interaction_date,\n",
    "                last_interaction_date\n",
    "            FROM bilateral_relationship_summaries\n",
    "            WHERE is_deleted = false\n",
    "            ORDER BY total_documents DESC\n",
    "            LIMIT 20\n",
    "        \"\"\")\n",
    "        print(\"\\nTop 20 Bilateral Relationships by Document Count:\")\n",
    "        display(bilateral_summaries)\n",
    "except Exception as e:\n",
    "    print(f\"Bilateral summaries table not available: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY CHECKS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Null checks for documents\n",
    "null_checks = run_query(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_docs,\n",
    "        SUM(CASE WHEN date IS NULL THEN 1 ELSE 0 END) as null_dates,\n",
    "        SUM(CASE WHEN initiating_country IS NULL THEN 1 ELSE 0 END) as null_init_country,\n",
    "        SUM(CASE WHEN recipient_country IS NULL THEN 1 ELSE 0 END) as null_rec_country,\n",
    "        SUM(CASE WHEN category IS NULL THEN 1 ELSE 0 END) as null_category,\n",
    "        SUM(CASE WHEN salience_bool IS NULL THEN 1 ELSE 0 END) as null_salience\n",
    "    FROM documents\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nNull Value Analysis (Documents):\")\n",
    "total = null_checks['total_docs'].iloc[0]\n",
    "print(f\"  Total documents: {total:,}\")\n",
    "print(f\"  Null dates: {null_checks['null_dates'].iloc[0]:,} ({null_checks['null_dates'].iloc[0]/total*100:.1f}%)\")\n",
    "print(f\"  Null initiating_country: {null_checks['null_init_country'].iloc[0]:,} ({null_checks['null_init_country'].iloc[0]/total*100:.1f}%)\")\n",
    "print(f\"  Null recipient_country: {null_checks['null_rec_country'].iloc[0]:,} ({null_checks['null_rec_country'].iloc[0]/total*100:.1f}%)\")\n",
    "print(f\"  Null category: {null_checks['null_category'].iloc[0]:,} ({null_checks['null_category'].iloc[0]/total*100:.1f}%)\")\n",
    "print(f\"  Null salience_bool: {null_checks['null_salience'].iloc[0]:,} ({null_checks['null_salience'].iloc[0]/total*100:.1f}%)\")\n",
    "\n",
    "# Orphan checks\n",
    "print(\"\\nOrphan Record Checks:\")\n",
    "try:\n",
    "    orphan_categories = run_scalar(\"\"\"\n",
    "        SELECT COUNT(*) FROM categories c\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM documents d WHERE d.doc_id = c.doc_id)\n",
    "    \"\"\")\n",
    "    print(f\"  Orphan categories: {orphan_categories:,}\")\n",
    "except:\n",
    "    print(\"  Orphan categories: N/A\")\n",
    "\n",
    "try:\n",
    "    orphan_mentions = run_scalar(\"\"\"\n",
    "        SELECT COUNT(*) FROM daily_event_mentions m\n",
    "        WHERE NOT EXISTS (SELECT 1 FROM canonical_events e WHERE e.id = m.canonical_event_id)\n",
    "    \"\"\")\n",
    "    print(f\"  Orphan daily mentions: {orphan_mentions:,}\")\n",
    "except:\n",
    "    print(\"  Orphan daily mentions: N/A\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"EXECUTIVE SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "summary = {\n",
    "    'metric': [],\n",
    "    'value': []\n",
    "}\n",
    "\n",
    "# Documents\n",
    "summary['metric'].append('Total Documents')\n",
    "summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM documents'):,}\")\n",
    "\n",
    "# Date range\n",
    "date_info = run_query(\"SELECT MIN(date), MAX(date) FROM documents WHERE date IS NOT NULL\")\n",
    "summary['metric'].append('Date Range')\n",
    "summary['value'].append(f\"{date_info.iloc[0, 0]} to {date_info.iloc[0, 1]}\")\n",
    "\n",
    "# Influencer coverage\n",
    "influencer_docs = run_scalar(f\"\"\"\n",
    "    SELECT COUNT(*) FROM documents \n",
    "    WHERE initiating_country IN {tuple(INFLUENCERS)}\n",
    "\"\"\")\n",
    "summary['metric'].append('Documents from Config Influencers')\n",
    "summary['value'].append(f\"{influencer_docs:,}\")\n",
    "\n",
    "# Recipient coverage\n",
    "recipient_docs = run_scalar(f\"\"\"\n",
    "    SELECT COUNT(*) FROM documents \n",
    "    WHERE recipient_country IN {tuple(RECIPIENTS)}\n",
    "\"\"\")\n",
    "summary['metric'].append('Documents to Config Recipients')\n",
    "summary['value'].append(f\"{recipient_docs:,}\")\n",
    "\n",
    "# Events\n",
    "try:\n",
    "    summary['metric'].append('Canonical Events')\n",
    "    summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM canonical_events'):,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    summary['metric'].append('Event Clusters')\n",
    "    summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM event_clusters'):,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    summary['metric'].append('Daily Event Mentions')\n",
    "    summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM daily_event_mentions'):,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    summary['metric'].append('Event Summaries')\n",
    "    summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM event_summaries'):,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "# Embeddings\n",
    "try:\n",
    "    summary['metric'].append('Total Embeddings')\n",
    "    summary['value'].append(f\"{run_scalar('SELECT COUNT(*) FROM langchain_pg_embedding'):,}\")\n",
    "except:\n",
    "    pass\n",
    "\n",
    "summary_df = pd.DataFrame(summary)\n",
    "print(\"\\n\")\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"AUDIT COMPLETE\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nGenerated at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}