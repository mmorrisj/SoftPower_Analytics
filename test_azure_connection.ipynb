{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure OpenAI Connection Test - System 2\n",
    "\n",
    "This notebook tests:\n",
    "1. Environment variable loading\n",
    "2. Azure OpenAI client initialization\n",
    "3. Basic LLM call\n",
    "4. JSON response parsing\n",
    "5. Materiality scoring format (pipeline simulation)\n",
    "\n",
    "---\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run this first to set up the environment and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to path\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.insert(0, str(project_root))\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"âœ… Environment setup complete\")\n",
    "print(f\"Project root: {project_root}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the gai function and utilities\n",
    "from shared.utils.utils import gai, initialize_client\n",
    "\n",
    "print(\"âœ… Imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 1: Check Environment Variables\n",
    "\n",
    "Verify that all required Azure OpenAI environment variables are set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 1: Environment Variables\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Check required variables\n",
    "required_vars = {\n",
    "    'AZURE_OPENAI_ENDPOINT': os.getenv('AZURE_OPENAI_ENDPOINT'),\n",
    "    'AZURE_OPENAI_API_KEY': os.getenv('AZURE_OPENAI_API_KEY'),\n",
    "}\n",
    "\n",
    "# Check optional variables\n",
    "optional_vars = {\n",
    "    'AZURE_OPENAI_API_VERSION': os.getenv('AZURE_OPENAI_API_VERSION', '2024-02-15-preview'),\n",
    "    'AZURE_OPENAI_DEPLOYMENT': os.getenv('AZURE_OPENAI_DEPLOYMENT', 'gpt-4o-mini'),\n",
    "}\n",
    "\n",
    "# Display required vars\n",
    "print(\"\\nRequired variables:\")\n",
    "all_set = True\n",
    "for var_name, var_value in required_vars.items():\n",
    "    if var_value:\n",
    "        # Mask the API key for security\n",
    "        if 'KEY' in var_name:\n",
    "            masked_value = var_value[:8] + '...' + var_value[-4:] if len(var_value) > 12 else '***'\n",
    "            print(f\"  âœ… {var_name}: {masked_value}\")\n",
    "        else:\n",
    "            print(f\"  âœ… {var_name}: {var_value}\")\n",
    "    else:\n",
    "        print(f\"  âŒ {var_name}: NOT SET\")\n",
    "        all_set = False\n",
    "\n",
    "# Display optional vars\n",
    "print(\"\\nOptional variables (with defaults):\")\n",
    "for var_name, var_value in optional_vars.items():\n",
    "    print(f\"  {var_name}: {var_value}\")\n",
    "\n",
    "# Summary\n",
    "if all_set:\n",
    "    print(\"\\nâœ… TEST 1 PASSED: All required environment variables are set\")\n",
    "else:\n",
    "    print(\"\\nâŒ TEST 1 FAILED: Missing required environment variables\")\n",
    "    print(\"\\nPlease add to your .env file:\")\n",
    "    print(\"  AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/\")\n",
    "    print(\"  AZURE_OPENAI_API_KEY=your-azure-api-key-here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 2: Azure OpenAI Client Initialization\n",
    "\n",
    "Test that we can successfully initialize the Azure OpenAI client."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 2: Client Initialization\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(\"\\nInitializing Azure OpenAI client (using environment variables)...\")\n",
    "    client = initialize_client(use_env_vars=True)\n",
    "    \n",
    "    print(f\"âœ… Client initialized successfully\")\n",
    "    print(f\"Client type: {type(client).__name__}\")\n",
    "    print(f\"\\nâœ… TEST 2 PASSED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Client initialization failed\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nâŒ TEST 2 FAILED\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 3: Simple LLM Call\n",
    "\n",
    "Test a basic call to Azure OpenAI to verify connectivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 3: Simple LLM Call\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(\"\\nSending test prompt to Azure OpenAI...\")\n",
    "    \n",
    "    response = gai(\n",
    "        sys_prompt=\"You are a helpful assistant.\",\n",
    "        user_prompt=\"Say 'Hello from Azure OpenAI!' and nothing else.\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        source=\"azure\",\n",
    "        azure_use_env=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… LLM call successful!\")\n",
    "    print(f\"Response type: {type(response).__name__}\")\n",
    "    print(f\"Response: {response}\")\n",
    "    print(f\"\\nâœ… TEST 3 PASSED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ LLM call failed\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nâŒ TEST 3 FAILED\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 4: JSON Response Parsing\n",
    "\n",
    "Test that JSON responses are correctly parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 4: JSON Response Parsing\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(\"\\nRequesting JSON response from Azure OpenAI...\")\n",
    "    \n",
    "    response = gai(\n",
    "        sys_prompt=\"You are a helpful assistant that responds in JSON format.\",\n",
    "        user_prompt=(\n",
    "            \"Respond with a JSON object with the following fields:\\n\"\n",
    "            \"- status: 'success'\\n\"\n",
    "            \"- message: 'Azure OpenAI is working correctly'\\n\"\n",
    "            \"- test_passed: true\\n\"\n",
    "            \"Respond ONLY with the JSON object, no additional text.\"\n",
    "        ),\n",
    "        model=\"gpt-4o-mini\",\n",
    "        source=\"azure\",\n",
    "        azure_use_env=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… JSON parsing successful!\")\n",
    "    print(f\"Response type: {type(response).__name__}\")\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        print(f\"\\nParsed JSON:\")\n",
    "        for key, value in response.items():\n",
    "            print(f\"  - {key}: {value}\")\n",
    "        \n",
    "        if response.get('status') == 'success':\n",
    "            print(f\"\\nâœ… TEST 4 PASSED: JSON validation successful\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  TEST 4 PARTIAL: JSON received but validation failed\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Response was not parsed as JSON\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        print(f\"\\nâŒ TEST 4 FAILED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ JSON parsing test failed\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nâŒ TEST 4 FAILED\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Test 5: Materiality Scoring Format\n",
    "\n",
    "Test the actual materiality scoring prompt format used in the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 5: Materiality Scoring Format\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Materiality scoring prompt (simplified version)\n",
    "materiality_sys_prompt = \"\"\"You are an expert analyst assessing the material impact of soft power events.\n",
    "\n",
    "**YOUR TASK:**\n",
    "Assign a materiality score from 1.0 to 10.0 measuring the concrete/substantive nature of this event.\n",
    "\n",
    "**Scoring Scale:**\n",
    "- 1-3: Symbolic/rhetorical (statements, cultural events with no material commitments)\n",
    "- 4-6: Mixed symbolic and material (agreements with unclear implementation)\n",
    "- 7-10: Highly material (concrete infrastructure, specific financial commitments)\n",
    "\n",
    "**Output JSON format:**\n",
    "{\n",
    "    \"material_score\": 7.5,\n",
    "    \"justification\": \"Brief explanation\"\n",
    "}\n",
    "\n",
    "Respond with ONLY the JSON object.\"\"\"\n",
    "\n",
    "materiality_user_prompt = \"\"\"**Event:** China-Egypt Belt and Road Infrastructure Agreement\n",
    "**Country:** China\n",
    "**Time Period:** 2024-08-15\n",
    "**Categories:** Economic, Infrastructure\n",
    "**Recipients:** Egypt\n",
    "\n",
    "**Event Description:**\n",
    "China and Egypt signed a comprehensive infrastructure development agreement under the Belt and Road Initiative, \n",
    "including $2.5 billion in financing for a new high-speed rail link and port expansion project. \n",
    "Construction is scheduled to begin in Q1 2025.\"\"\"\n",
    "\n",
    "try:\n",
    "    print(\"\\nTesting materiality scoring prompt format...\")\n",
    "    \n",
    "    response = gai(\n",
    "        sys_prompt=materiality_sys_prompt,\n",
    "        user_prompt=materiality_user_prompt,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        source=\"azure\",\n",
    "        azure_use_env=True\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… Materiality scoring test successful!\")\n",
    "    print(f\"Response type: {type(response).__name__}\")\n",
    "    \n",
    "    if isinstance(response, dict):\n",
    "        score = response.get('material_score')\n",
    "        justification = response.get('justification', '')\n",
    "        \n",
    "        print(f\"\\nParsed materiality response:\")\n",
    "        print(f\"  - Material Score: {score}/10.0\")\n",
    "        print(f\"  - Justification: {justification[:150]}...\" if len(justification) > 150 else f\"  - Justification: {justification}\")\n",
    "        \n",
    "        if isinstance(score, (int, float)) and 1.0 <= score <= 10.0:\n",
    "            print(f\"\\nâœ… TEST 5 PASSED: Materiality scoring format validation successful\")\n",
    "            print(f\"\\nðŸŽ‰ Your Azure OpenAI setup is ready for the materiality scoring pipeline!\")\n",
    "        else:\n",
    "            print(f\"\\nâš ï¸  Score validation failed (expected 1.0-10.0, got {score})\")\n",
    "            print(f\"\\nâŒ TEST 5 FAILED\")\n",
    "    else:\n",
    "        print(f\"\\nâš ï¸  Response was not parsed as JSON\")\n",
    "        print(f\"Raw response: {response}\")\n",
    "        print(f\"\\nâŒ TEST 5 FAILED\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâŒ Materiality scoring test failed\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nâŒ TEST 5 FAILED\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary\n",
    "\n",
    "Review all test results below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\"\"\n",
    "If all tests passed:\n",
    "  âœ… Your Azure OpenAI configuration is working correctly\n",
    "  âœ… You can now run the summary and GAI pipeline on System 2\n",
    "  âœ… The gai() function is ready to use with source=\"azure\" and azure_use_env=True\n",
    "\n",
    "Next steps:\n",
    "  1. Update pipeline scripts to use: gai(..., source=\"azure\", azure_use_env=True)\n",
    "  2. Test specific pipeline components (bilateral summaries, event summaries, etc.)\n",
    "  3. Run production workloads\n",
    "\n",
    "If tests failed:\n",
    "  - Review error messages above\n",
    "  - Verify .env file configuration\n",
    "  - Check Azure OpenAI resource settings\n",
    "  - Ensure deployment name matches your Azure setup\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Optional: Test AWS Secrets Manager Mode\n",
    "\n",
    "If you want to test the AWS Secrets Manager credential approach instead, run this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"OPTIONAL: Testing AWS Secrets Manager Mode\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "try:\n",
    "    print(\"\\nAttempting to use AWS Secrets Manager for credentials...\")\n",
    "    \n",
    "    response = gai(\n",
    "        sys_prompt=\"You are a helpful assistant.\",\n",
    "        user_prompt=\"Say 'AWS Secrets Manager mode is working!' and nothing else.\",\n",
    "        model=\"gpt-4o-mini\",\n",
    "        source=\"azure\",\n",
    "        azure_use_env=False  # Use AWS Secrets Manager\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nâœ… AWS Secrets Manager mode successful!\")\n",
    "    print(f\"Response: {response}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"\\nâš ï¸  AWS Secrets Manager mode failed (this is expected if not configured)\")\n",
    "    print(f\"Error: {e}\")\n",
    "    print(f\"\\nThis is OK - you can use environment variables mode (azure_use_env=True) instead\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
